{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum up :\n",
    "\n",
    "So what are problems if you have small data? Surely, you can use same methods and models. Well, small data exacerbates certain issues, like…\n",
    "\n",
    "    Outliers – Outlier handling is important for many models, but can be lived with if proportion of outliers is small. This is obviously not the case with small data since even few outliers will form large proportion and significantly alter the model.\n",
    "    Train and Test Data – A good design choice in model training is to split the data on which model is trained (“train data”) and report generalized performance on unseen data (“test data” or “holdout data”). In case, holdout data is used for tuning model parameters (sometimes called “cross-validation data”), you may need to split all observations into three sets. With small data, one doesn’t have luxury to keep out many samples, and even when one does so, number of observations in test data may be too few to give meaningful performance estimate, and/or number of observations in cross-validation data may be too few to guide parameter search optimally.\n",
    "    \n",
    "    Overfitting – If your training dataset itself is small, overfitting is more likely to occur. And using cross-validation to reduce overfitting has risk mentioned above.\n",
    "    Measurement Errors – Each metric, either a predictor or target, is measured in real world, and has associated measurement error. At small scale, effects of such errors become important and affect the model adversely.\n",
    "    \n",
    "    Missing Values – Missing values in data has effect in similar direction as of measurement errors, but perhaps more in magnitude. Limited number of observations means that imputing missing values can be difficult. Further, if target has missing values then whole observation may have to be dropped which is not desirable in such cases.\n",
    "    \n",
    "    Sampling Bias – Problem with small data can be worse if data is biased and not sampled randomly from population. This is often problem in sociology research, if not controlled in design, where test subjects are often people in same circle or environment as the researcher, say, undergraduate students of the college in which researcher practices.\n",
    "\n",
    "### How to handle them\n",
    "\n",
    "    Data review – Since abnormal data values impact predictive capacity more for small data, spend time in reviewing, cleaning, and managing your data. This means detecting outliers, imputing missing values or deciding how to use them, and understanding impact of measurement errors.\n",
    "    \n",
    "    Simpler models – Lesser the degrees of freedom compared to number of training observations, more robust are parameter estimates. Prefer simpler models when possible and limit number of parameters to be estimated. This means going for Logistics Regression rather than Neural Network, or k-Nearest Neighbours rather than Regression Splines. Use simplifying assumptions, such as those that favour Linear Discriminant Analysis over Quadratic Discriminant Analysis.\n",
    "    \n",
    "    Domain expertise – Use prior experience and domain expertise to decide on model form. Small data doesn’t offer luxury of testing different model forms and hence expert opinion counts more. Use domain knowledge to design features effectively and do feature selection. We cannot afford to throw all possible features in the mix and let the model figure out right set.\n",
    "    Consortium approach – Build and grow your data over time and across sectors. Even small data adds up over time. Using slightly unrelated data to increase number of observations and then subtracting impact of unrelated-ness mathematically can still produce better performing models. For example, use Panel Regression instead of separate Linear Regressions for different groups within the data.\n",
    "    \n",
    "    Ensemble approach – Build multiple simple models rather than one best model, and use bagging or stacking approach. Ensemble models tend to reduce overfitting without increasing number of parameters to be estimated.\n",
    "    No cross-validation data – This extends idea of simpler models. Don’t over-use cross-validation data for hyper-parameter optimization. If number of observations is really small, do not use cross-validation data for model training.\n",
    "    \n",
    "    Regularization – Regularization is way to produce more robust parameter estimates and is very useful in small data space. While regularization does add one more parameter to modeling process, often this increase is worthwhile. Lasso or L1 regularization produces fewer non-zero parameters and indirectly does feature selection. Ridge or L2 regularization produces smaller (in absolute value) and conservative coefficient estimates.\n",
    "    \n",
    "    Confidence intervals – Try to predict with margin of error, rather than point estimates. Models on small data will have large confidence intervals but it is better to be aware of range when making actionable decisions on predictions rather than not know.\n",
    "    \n",
    "# Synthetic Data\n",
    "\n",
    "Synthetic data means fake data that contains the same schema and statistical properties as its “real” counterpart. Basically, it looks so real that it’s nearly impossible to tell that it’s not.\n",
    "\n",
    "So what’s the point of synthetic data, and why does it matter if we already have access to the real thing?\n",
    "\n",
    "I have seen synthetic data applied, especially when we were dealing with private data (banking, healthcare, etc.), which makes the use of synthetic data a more secure approach to development in certain instances.\n",
    "\n",
    "Synthetic data is used mostly when there is not enough real data, or there is not enough real data for specific patterns you know about. Its usage is mostly the same for training and testing datasets.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE) and Modified-SMOTE are two techniques which generate synthetic data. Simply put, SMOTE takes the minority class data points and creates new data points that lie between any two nearest data points joined by a straight line.\n",
    "\n",
    "The algorithm calculates the distance between two data points in the feature space, multiplies the distance by a random number between 0 and 1, and places the new data point at this new distance from one of the data points used for distance calculation.\n",
    "\n",
    "In order to generate synthetic data, you have to use a training set to define a model, which would require validation, and then by changing the parameters of interest, you can generate synthetic data, through simulation. The domain/data type is significant since it affects the complexity of the entire process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection and Improvement Suggestions:\n",
    "\n",
    "\n",
    "Upon reflection, I would have given every feature its chance in predicting interest rates and used a more empirical of statistical techniques in evaluating features for preprocessing, like using extreme gradient boosting F-scores to detect and display the most important features. From there I could potentially choose more important features and omit less important features, as well as engineer new features out of highly important features to create stronger signals. For example, I would look futher into the relationship between loan risque and interest rate by generating a scatter plot of the two variables to discern if there is a specific relationship between the two \n",
    "-Collect more data .\n",
    "-Collect historical behavior of each client \n",
    "-Use nlp indocator of each company .\n",
    "-Handle the outlier with Domain expert in finance .\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
